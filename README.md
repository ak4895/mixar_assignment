# 3D Mesh Normalization and Quantization Pipeline

## Project Overview

This project implements a complete pipeline for 3D mesh normalization, quantization, and reconstruction. It processes `.obj` mesh files through multiple normalization methods, quantizes the coordinates into discrete bins, and evaluates reconstruction quality with comprehensive error metrics and visualizations.

**Built with a systematic step-by-step approach** - breaking down a complex problem into 8 manageable, sequential tasks that build upon each other.

## Author & Date

- **Date:** November 10, 2025
- **Repository:** Bandook

## Problem-Solving Methodology

### The Step-by-Step Approach

This project demonstrates a **divide-and-conquer strategy** for solving complex computational problems:

#### ğŸ¯ Core Philosophy
**"Break down complexity into smaller, conquerable tasks"**

Rather than attempting to solve the entire mesh processing pipeline at once, this project **systematically decomposes** the challenge into **8 distinct, manageable steps**, where each step:
- âœ… Has a **clear, singular objective**
- âœ… Produces **verifiable outputs**
- âœ… Can be **tested independently**
- âœ… **Builds upon** previous steps
- âœ… **Enables** subsequent steps

#### ğŸ“Š Benefits of This Approach

1. **Reduced Complexity** - Each step solves one specific problem
2. **Easier Debugging** - Issues can be isolated to specific steps
3. **Incremental Progress** - See results after each step completion
4. **Modular Design** - Steps can be modified without affecting others
5. **Clear Dependencies** - Understand what each step requires
6. **Reproducible Results** - Each step can be re-run independently
7. **Scalability** - Additional meshes or methods can be added easily
8. **Maintainability** - Code is organized and easy to understand

#### ğŸ”„ Pipeline Architecture

```
Input (.obj files) 
    â†“
Step 1: Setup & Validation â†’ [Environment Ready]
    â†“
Step 2: Data Extraction â†’ [Vertex Statistics]
    â†“
Step 3: Normalization â†’ [Transform Functions]
    â†“
Step 4: Quantization â†’ [Integer Bins]
    â†“
Step 5: Reconstruction â†’ [Recovered Meshes]
    â†“
Step 6: Error Analysis â†’ [Quality Metrics]
    â†“
Step 7: Visualization â†’ [Visual Comparisons]
    â†“
Step 8: Documentation â†’ [Final Report]
    â†“
Output (Complete Analysis)
```

Each arrow represents a **validated checkpoint** - you can't proceed until the previous step succeeds.

## Table of Contents

1. [Requirements](#requirements)
2. [Installation](#installation)
3. [Project Structure](#project-structure)
4. [Usage](#usage)
5. [Pipeline Steps](#pipeline-steps)
6. [Output Files](#output-files)
7. [Normalization Methods](#normalization-methods)
8. [Results Summary](#results-summary)

## Requirements

### Python Version
- Python >= 3.8

### Required Libraries
```
numpy
trimesh
open3d
matplotlib
```

## Installation

### Step 1: Install Required Packages

```bash
pip install numpy trimesh open3d matplotlib
```

### Step 2: Verify Installation

Run Step 1 to verify all dependencies:

```bash
python src/step1_load.py
```

## Project Structure

```
Bandook/
â”œâ”€â”€ 8samples/              # Input directory containing .obj mesh files
â”‚   â”œâ”€â”€ branch.obj
â”‚   â”œâ”€â”€ cylinder.obj
â”‚   â”œâ”€â”€ explosive.obj
â”‚   â”œâ”€â”€ fence.obj
â”‚   â”œâ”€â”€ girl.obj
â”‚   â”œâ”€â”€ person.obj
â”‚   â”œâ”€â”€ table.obj
â”‚   â””â”€â”€ talwar.obj
â”œâ”€â”€ src/                   # Source code directory
â”‚   â”œâ”€â”€ step1_load.py                 # Environment setup and file listing
â”‚   â”œâ”€â”€ step2_extract_vertices.py     # Load meshes and extract statistics
â”‚   â”œâ”€â”€ normalization.py              # Normalization functions (Min-Max & Sphere)
â”‚   â”œâ”€â”€ step4_quantize.py             # Quantization to integer bins
â”‚   â”œâ”€â”€ step5_reconstruct.py          # Dequantization and denormalization
â”‚   â”œâ”€â”€ step6_metrics.py              # Error metrics and visualizations
â”‚   â”œâ”€â”€ step7_visualize.py            # Mesh visualizations
â”‚   â””â”€â”€ step8_package.py              # Final packaging and report
â”œâ”€â”€ outputs/               # All output files
â”‚   â”œâ”€â”€ *_stats.json                  # Per-mesh statistics
â”‚   â”œâ”€â”€ *_quantized_*.npy             # Quantized integer arrays
â”‚   â”œâ”€â”€ *_quantized_*.ply             # Quantized meshes
â”‚   â”œâ”€â”€ *_params_*.json               # Normalization parameters
â”‚   â”œâ”€â”€ *_reconstructed_*.ply         # Reconstructed meshes
â”‚   â”œâ”€â”€ *_metrics_*.json              # Error metrics
â”‚   â”œâ”€â”€ *_mse_axis_*.png              # MSE per-axis plots
â”‚   â”œâ”€â”€ *_error_hist_*.png            # Error distribution histograms
â”‚   â”œâ”€â”€ *_error_sorted_*.png          # Sorted error plots
â”‚   â”œâ”€â”€ comparison_summary.txt        # Summary comparison table
â”‚   â””â”€â”€ visuals/                      # Visualization plots
â”‚       â”œâ”€â”€ *_original_info.png
â”‚       â”œâ”€â”€ *_reconstructed_*_info.png
â”‚       â”œâ”€â”€ *_comparison_*.png
â”‚       â””â”€â”€ summary_comparison.png
â”œâ”€â”€ logs/                  # Log files for each step
â”œâ”€â”€ README.md              # This file
â””â”€â”€ report.pdf             # Final comprehensive report
```

## Usage

### Running Individual Steps

Execute each step in sequence:

```bash
# Step 1: Environment setup and file discovery
python src/step1_load.py

# Step 2: Load meshes and extract vertex statistics
python src/step2_extract_vertices.py

# Step 3: Normalization methods (tested automatically)
python src/normalization.py

# Step 4: Quantize meshes (both Min-Max and Sphere methods)
python src/step4_quantize.py

# Step 5: Reconstruct meshes from quantized data
python src/step5_reconstruct.py

# Step 6: Compute error metrics and generate plots
python src/step6_metrics.py

# Step 7: Create visualizations
python src/step7_visualize.py

# Step 8: Package and create final report
python src/step8_package.py
```

### Running Complete Pipeline

To run all steps end-to-end:

```bash
python src/step1_load.py && python src/step2_extract_vertices.py && python src/step4_quantize.py && python src/step5_reconstruct.py && python src/step6_metrics.py && python src/step7_visualize.py && python src/step8_package.py
```

## Pipeline Steps

### ğŸ¯ The Step-by-Step Breakdown

Each step is designed as a **focused micro-project** that solves one specific challenge. This modular approach makes the complex problem manageable and the solution robust.

---

### Step 1: Environment & Inputs âœ… Foundation
**Objective:** Ensure the workspace is ready before starting any processing

**Breaking it down:**
- âœ“ Check if all required libraries are installed
- âœ“ Create necessary output directories
- âœ“ Verify input files exist and are accessible
- âœ“ Log the initial setup state

**Why this step matters:** Prevents runtime failures later. Catches environment issues early when they're easy to fix.

**Output:** Confirmation of setup and file count

---

### Step 2: Load Mesh and Extract Vertices ğŸ“Š Data Acquisition
**Objective:** Understand the input data before processing

**Breaking it down:**
- âœ“ Load each `.obj` file using `trimesh.load(path, process=False)`
- âœ“ Extract vertex arrays (shape: V Ã— 3)
- âœ“ Compute per-axis statistics: min, max, mean, std
- âœ“ Save statistics for later reference

**Why this step matters:** You can't normalize what you don't understand. Statistics reveal data characteristics and inform normalization strategy.

**Output:** `*_stats.json` for each mesh

---

### Step 3: Normalization Methods ğŸ”„ Transform Design
**Objective:** Create reusable transformation functions

**Breaking it down:**

#### Min-Max Normalization
- Maps coordinates to [0, 1] range per axis
- Formula: `normalized = (vertices - v_min) / (v_max - v_min)`
- Stores: `v_min`, `v_max` for denormalization

#### Unit Sphere Normalization
- Centers mesh at origin and scales to unit sphere
- Process: center â†’ shift â†’ compute max radius â†’ normalize
- Formula: `normalized = (vertices - center) / scale`
- Stores: `center`, `scale` for denormalization

**Breaking it down:**
- âœ“ Design Min-Max normalization (axis-independent scaling to [0,1])
- âœ“ Design Unit Sphere normalization (uniform scaling to unit sphere)
- âœ“ Implement forward transforms (original â†’ normalized)
- âœ“ Implement inverse transforms (normalized â†’ original)
- âœ“ Test both methods for mathematical correctness
- âœ“ Handle edge cases (zero ranges, degenerate meshes)

**Why this step matters:** Normalization is the foundation of quantization. Both forward and inverse must be mathematically exact for accurate reconstruction.

#### Min-Max Normalization
- Maps coordinates to [0, 1] range per axis
- Formula: `normalized = (vertices - v_min) / (v_max - v_min)`
- Stores: `v_min`, `v_max` for denormalization

#### Unit Sphere Normalization
- Centers mesh at origin and scales to unit sphere
- Process: center â†’ shift â†’ compute max radius â†’ normalize
- Formula: `normalized = (vertices - center) / scale`
- Stores: `center`, `scale` for denormalization

**Output:** `normalization.py` module with reusable functions

---

### Step 4: Quantize ğŸ² Discretization
**Objective:** Convert continuous coordinates to discrete integer bins

**Breaking it down:**
- âœ“ Set quantization parameter: BINS = 1024 (range: 0-1023)
- âœ“ Apply normalization to each mesh
- âœ“ Quantize normalized coordinates: `q = floor(normalized * 1023)`
- âœ“ Handle method-specific mappings (Min-Max vs Sphere)
- âœ“ Save quantized arrays, parameters, and sample meshes
- âœ“ Verify quantized values are within valid range

**Why this step matters:** Quantization introduces controlled information loss. This step measures and controls that loss through proper bin selection.

- **Min-Max:** Already in [0,1] â†’ `q = floor(normalized * 1023)`
- **Sphere:** Map [-1,1] to [0,1] first, then quantize

**Output:** `*_quantized_*.npy`, `*_quantized_*.ply`, `*_params_*.json`

---

### Step 5: Reconstruct ğŸ”¨ Inverse Transform
**Objective:** Recover original-scale meshes from quantized data

**Breaking it down:**
- âœ“ Load quantized integer arrays
- âœ“ Load normalization parameters
- âœ“ Dequantize: Integer bins â†’ normalized coordinates
- âœ“ Denormalize: Normalized â†’ original world coordinates
- âœ“ Validate shapes match original meshes
- âœ“ Save reconstructed meshes

**Why this step matters:** Tests the entire pipeline. If reconstruction fails, it reveals issues in normalization or quantization.

**Output:** `*_reconstructed_*.ply` meshes

---

### Step 6: Compute Error Metrics ğŸ“ˆ Quality Assessment
**Objective:** Quantify reconstruction quality objectively

**Breaking it down:**
- âœ“ Load original vertices
- âœ“ Load reconstructed vertices
- âœ“ Compute error: `error = |original - reconstructed|`
- âœ“ Calculate MSE (Mean Squared Error): Total and per-axis
- âœ“ Calculate MAE (Mean Absolute Error): Total and per-axis
- âœ“ Compute per-vertex error statistics: Max, min, std
- âœ“ Generate error visualizations (bar charts, histograms, line plots)
- âœ“ Save all metrics as JSON

**Why this step matters:** Numbers don't lie. Objective metrics reveal which method works better and by how much.

**Visualizations:**
- Bar charts: MSE per axis
- Histograms: Error distribution
- Line plots: Sorted errors

**Output:** `*_metrics_*.json`, error plots (PNG)

---

### Step 7: Visualization ğŸ¨ Visual Inspection
**Objective:** Create human-readable visual comparisons

**Breaking it down:**
- âœ“ Create original mesh info plots (statistics + projections)
- âœ“ Create reconstructed mesh info plots
- âœ“ Generate side-by-side comparison plots (colored by error)
- âœ“ Build summary comparison across all meshes
- âœ“ Use color maps to highlight error magnitudes
- âœ“ Include statistical annotations

**Why this step matters:** Humans spot patterns machines miss. Visualizations reveal spatial error distribution and validate numerical metrics.

**Output:** 41 visualization PNG files in `outputs/visuals/`

---

### Step 8: Packaging & Report ğŸ“¦ Documentation
**Objective:** Create comprehensive documentation of results

**Breaking it down:**
- âœ“ Load all metrics from previous steps
- âœ“ Generate comparison tables
- âœ“ Create final report with findings
- âœ“ Document best-performing method
- âœ“ Update README with results
- âœ“ Create file manifest
- âœ“ Optionally package everything into ZIP

**Why this step matters:** Great work means nothing if it's not documented. This step ensures results are understandable and reproducible.

**Output:** README.md, REPORT.txt, FILE_MANIFEST.txt

---

### ğŸ’¡ Key Insight: Why This Approach Works

**Traditional approach:** Write one big script that does everything
- âŒ Hard to debug when something fails
- âŒ Can't test individual components
- âŒ Changes break everything
- âŒ No visibility into intermediate results

**Step-by-step approach:** Break into 8 independent modules
- âœ… **Isolation** - Each step can be tested separately
- âœ… **Visibility** - See results after each step
- âœ… **Debugging** - Know exactly where issues occur
- âœ… **Flexibility** - Modify one step without breaking others
- âœ… **Confidence** - Each success builds toward the solution
- âœ… **Learning** - Understand each component deeply

**Result:** A robust, maintainable, and understandable solution to a complex problem.

## Output Files

### Per-Mesh Outputs (8 meshes Ã— 2 methods = 16 sets)

1. **Statistics:** `<mesh>_stats.json`
2. **Quantized arrays:** `<mesh>_quantized_<method>.npy`
3. **Quantized meshes:** `<mesh>_quantized_<method>.ply`
4. **Parameters:** `<mesh>_params_<method>.json`
5. **Reconstructed meshes:** `<mesh>_reconstructed_<method>.ply`
6. **Metrics:** `<mesh>_metrics_<method>.json`
7. **Plots:** `<mesh>_mse_axis_<method>.png`, `<mesh>_error_hist_<method>.png`, `<mesh>_error_sorted_<method>.png`

### Summary Outputs

- `comparison_summary.txt` - Tabular comparison of all results
- `summary_comparison.png` - Visual comparison chart
- Log files in `logs/` directory for each step

## Normalization Methods

### Method Comparison

| Aspect | Min-Max Normalization | Unit Sphere Normalization |
|--------|----------------------|---------------------------|
| **Range** | [0, 1] per axis | Within unit sphere (radius â‰¤ 1) |
| **Preservation** | Axis-independent scaling | Uniform scaling from centroid |
| **Quantization** | Uses full bin range [0, 1023] | Uses subset of bins |
| **Reconstruction** | Generally lower error | Higher error due to method |
| **Best for** | Axis-aligned objects | Spherical/centered objects |

### Implementation Notes

1. **Consistency:** Both forward and inverse transforms are implemented with exact mathematical precision
2. **Edge cases:** Handles division by zero, single points, degenerate meshes
3. **Parameters:** All normalization parameters are saved as JSON for exact reconstruction
4. **Precision:** Uses `float64` for computations to minimize numerical errors

## Results Summary

### Reconstruction Quality (MSE)

| Mesh | Min-Max MSE | Sphere MSE | Best Method |
|------|-------------|------------|-------------|
| branch | 0.00000078 | 0.00000234 | Min-Max |
| cylinder | 0.00000080 | 0.00000257 | Min-Max |
| explosive | 0.00000012 | 0.00000043 | Min-Max |
| fence | 0.00000016 | 0.00000036 | Min-Max |
| girl | 0.00000021 | 0.00000036 | Min-Max |
| person | 0.00000079 | 0.00000179 | Min-Max |
| table | 0.00000015 | 0.00000047 | Min-Max |
| talwar | 0.00000013 | 0.00000060 | Min-Max |

### Key Findings

1. **Min-Max consistently outperforms Unit Sphere** - Lower MSE/MAE across all meshes (2-3Ã— better)
2. **All reconstructions are high quality** - MSE well below threshold (0.001)
3. **Best reconstruction:** explosive.obj (MSE: 0.00000012)
4. **Quantization is very effective** - 1024 bins provides excellent precision
5. **No visual distortions** - Reconstruction errors are sub-millimeter scale

### Why Min-Max Performs Better

1. **Full bin utilization:** Uses entire [0, 1023] range per axis
2. **Axis-independent:** Preserves original axis proportions
3. **No wasted precision:** Each axis gets maximum quantization resolution
4. **Better for axis-aligned meshes:** Most test meshes are axis-aligned

## Notes

- All meshes maintain original face topology during reconstruction
- Quantization introduces controlled loss - all errors measured and documented
- Visualizations show error distribution is uniform (no systematic bias)
- Pipeline is fully deterministic and reproducible

## Lessons Learned: The Power of Decomposition

### ğŸ§© Problem Decomposition Strategy

This project exemplifies how to tackle complex computational problems:

#### 1. **Identify the End Goal**
- Clear objective: Quantize 3D meshes and measure reconstruction quality
- Success criteria: Low reconstruction error, comprehensive analysis

#### 2. **Work Backwards**
- To measure quality â†’ need reconstructed meshes
- To reconstruct â†’ need quantized data
- To quantize â†’ need normalized coordinates
- To normalize â†’ need original vertices
- To get vertices â†’ need loaded meshes
- To load â†’ need verified environment

#### 3. **Define Dependencies**
```
Step 8 depends on â†’ Steps 6, 7
Step 7 depends on â†’ Steps 2, 5, 6
Step 6 depends on â†’ Steps 2, 5
Step 5 depends on â†’ Step 4
Step 4 depends on â†’ Steps 2, 3
Step 3 depends on â†’ Nothing (pure functions)
Step 2 depends on â†’ Step 1
Step 1 depends on â†’ Nothing (setup)
```

#### 4. **Build in Order**
- Start with no dependencies (Steps 1, 3)
- Progress through dependency chain
- Test each step before moving to the next

#### 5. **Validate at Each Stage**
- Step outputs become next step's inputs
- Checkpoints prevent cascading failures
- Early validation saves debugging time

### ğŸ“ Transferable Skills

This systematic approach applies to **any** complex problem:

- **Data Science Projects** - ETL â†’ Analysis â†’ Modeling â†’ Evaluation â†’ Deployment
- **Web Applications** - Database â†’ Backend â†’ API â†’ Frontend â†’ Testing
- **Machine Learning** - Data Collection â†’ Preprocessing â†’ Training â†’ Validation â†’ Inference
- **Research Projects** - Literature Review â†’ Hypothesis â†’ Experiment â†’ Analysis â†’ Publication

### ğŸ”‘ Key Principles Applied

1. **Single Responsibility** - Each step does ONE thing well
2. **Clear Interfaces** - Steps communicate through well-defined files/formats
3. **Error Handling** - Validate inputs before processing
4. **Logging** - Track progress and results at each stage
5. **Repeatability** - Same inputs â†’ same outputs
6. **Documentation** - Each step explains what and why

### ğŸš€ Scalability Benefits

Because of the modular design:
- âœ… **Add new meshes** - Just run the pipeline on new .obj files
- âœ… **Add new methods** - Implement in Step 3, automatically flows through
- âœ… **Change quantization** - Modify BINS parameter in one place
- âœ… **Add metrics** - Extend Step 6 without touching other steps
- âœ… **Improve visualizations** - Update Step 7 independently

**The step-by-step approach transforms complexity into simplicity.**

## Troubleshooting

### "No .obj files found"
- Ensure meshes are in `8samples/` directory
- Check file extension is `.obj` (lowercase)

### "Module not found" errors
- Run: `pip install numpy trimesh open3d matplotlib`
- Verify Python version >= 3.8

### "Shape mismatch" errors
- Run steps in sequence (Step 4 before Step 5, etc.)
- Delete `outputs/` directory and rerun pipeline

## License

This project is created for educational purposes.

## Contact

For questions or issues, please refer to the repository documentation.
